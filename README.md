# Multi-modal Long-Term User Recognition Dataset and Multi-modal Incremental Bayesian Network Evaluations

The datasets and the evaluations of the Multi-modal Incremental Bayesian Network (Irfan et al., 2018), [http://doc.aldebaran.com/2-5/naoqi/](NAOqi) face recognition and [https://github.com/EMRResearch/ExtremeValueMachine](Extreme Value Machine) (Rudd et al., 2017) are described in the paper below:

 * Bahar Irfan, Michael Garcia Ortiz, Natalia Lyubova, and Tony Belpaeme (under review), "Multi-modal Incremental Bayesian Network with Online Learning for Open World User Identification", ACM Transactions on Human-Robot Interaction (THRI).

The Multi-modal Incremental Bayesian Network (for the most up-to-date version with necessary libraries: https://github.com/birfan/MultimodalRecognition ) is also described in detail in the above paper and the following paper:

 * Bahar Irfan, Natalia Lyubova, Michael Garcia Ortiz, and Tony Belpaeme (2018), "Multi-modal Open-Set Person Identification in HRI", 2018 ACM/IEEE International Conference on Human-Robot Interaction Social Robots in the Wild workshop.

Please cite both papers and Gonzales et al. (2017) if you are using the Multi-modal Incremental Bayesian Network; cite the first paper for the Multi-modal Long-Term User Recognition Dataset; cite the first paper and Rudd et al. (2017) for the evaluations on the dataset; cite the second paper if you are referring to the real-world long-term human-robot interaction (HRI) experiment for user recognition.

## Multi-modal Long-Term User Recognition Dataset

Due to the size constraints of GitHub, the dataset is provided in *dataset* release. The trained models are available in *trained-models* release. The code to create the dataset and reproduce the experiments in Irfan et al. (under review) is available through:

    $ git clone https://github.com/birfan/MultimodalRecognitionDataset.git

Multi-modal Long-Term User Recognition Dataset contains images and corresponding face recognition similarity scores, gender and age estimations, along with simulated height and time of interaction for 200 users. The images are taken from [https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/](IMDB-WIKI dataset) (Rothe et al., 2015; Rothe et al., 2018) which contains cropped faces of images of celebrities, taken at events or still frames from movies.The proprietary algorithms of the Pepper robot (SoftBank Robotics Europe) were used to obtain multi-modal biometric information from these images (face, gender and age), while the height and time of interaction are artificially generated to simulate a long-term HRI scenario. 

The 200 users in the dataset are randomly sampled out of 20k celebrities, choosing only celebrities which have more than 10 images each corresponding to the same age, using *imdb_face_crossval_extraction.m* script. The resulting dataset contains 101 females, 98 males and one transgender person. In the dataset, each image of the user was chosen from the same year in order to simulate an open world HRI scenario, where the users will be met in consecutive days or weeks. The images that correspond to an age that is within the five most common ages in the set were randomly rejected during the selection. The resulting age range is 10-63, with the mean age of 33.04 (standard deviation (SD) is 9.28). To keep the data realistic and model the differences between the estimated heights, Gaussian noise with SD=6.3 cm found in (Irfan et al., 2018) added to the heights obtained from the web. Each image has a resolution greater than (or equal to) 150x150, has a single face in the image that corresponds to the correct celebrity (i.e., the IMDB-Wiki dataset was cleaned to ensure these criteria are met, using *imdb_prepareImages.py* script).

Based on the frequency of user encounters, we created the following datasets: (1) *D-Ten*, where each user is observed precisely ten times, e.g., ten return visits to a robot therapist, and (2) *D-All*, in which each user is encountered a different amount of times (10 to 41 times). Two types of distribution are considered for the time of interaction: (1) patterned interaction times in a week modelled through a Gaussian mixture model (*Gaussian*), where the user will be encountered certain times on specific days, which applies to HRI in rehabilitation and education areas, and (2) random interaction times represented by a uniform distribution (*Uniform*), such as in domestic applications with companion robots, where the user can be seen at any time of the day in the week, resulting in a total of four datasets (in *N10_gaussianT*, *N10_uniformT*, *Nall_gaussianT*, *Nall_uniformT* folders). The only difference between Gaussian and uniform datasets is the time of the interaction for each sample.

Each dataset is divided into two with 100 users each (for training and open-set evaluations). The first set is then divided through cross-validation procedure (in *cross_validation_train* folder) with 80% of the data for the *training* set (first four bins, corresponding to 800 samples in *D-Ten* and 2308 in *D-All*) and 20% of the data for the test set, *closed-set (training)* (final bin, corresponding to 200 samples in *D-Ten*, 620 in *D-All*). The *open-set* is created from the remaining 100 users (800 samples in *D-Ten*, 2280 in *D-All*). The *closed-set (open)* is similar to *closed-set (training)*, which corresponds to the final bin in each fold (200 in *D-Ten*, 570 in *D-All*). The open-set evaluation is made by introducing the *open-set* samples after the *training* set, that is, 100 users are enrolled in the system, and recognised multiple times before the introduction of 100 new users. However, the results for the *open-set* do not include the results for *training*. The models are trained on *training* set, and evaluated on *closed-set (training)*; afterwards, trained on *open-set* and evaluated on *closed-set (open)*.

The selected images from the IMDB-Wiki dataset are provided in *IMDB_chosen_images* folder, where *clean_original* folder contains the chosen users' images with the original ID used in the IMDB-Wiki dataset, along with the gender and age obtained from that dataset, in addition to heights of the users obtained from the web. The images are then processed with new IDs, and sequential ordering of images is conducted per user to generate *TrainAll* (10-41 images per user in training set), *TestAll* (10-41 images per user in open-set), *TrainTen* (10 images per user in training set), *TestTen* (10 images per user in open-set) folders, which are used for cross-validations as described below.
 
We evaluated the stability and performance of the user recognition models using repeated (11 repeats) 5-fold cross-validation. A stratified random bin order is used for having a different initial bin and final bin in each fold to ensure a different enrolment order of users and a different test set, respectively. The first repeat (i.e., repeat 1) is ordered, i.e., users are introduced one by one to the system without any repetitions of previous users during the enrolment. The remaining (10) repeats are shuffled, i.e., there can be repetitions of the previous user(s) before another user is introduced, because the order of overall samples is random. In other words, ordered repeat is similar to batch learning in an incremental learning sense, whereas, the shuffled iterations are more similar to a real-world scenario. This allows evaluating whether there are any performance differences between the two cases and to prove that the model is stable across several repeats. *crossValidation.py* script contains the code to generate the cross-validation sets.

These datasets are provided in *before_recognition* folder for training evaluations (*cross_validation_train*) and open-set evaluations (*cross_validation_open*). Within each fold, there is a *Training* folder that corresponds to *training* set in *cross_validation_train* (and *open-set* in *cross_validation_open*), and a *Test* folder, which corresponds to a *closed-set*. Within each fold, the *validation_info_fold.csv* carries the image path, artificially generated height estimation and time of interaction, and ID of user, as follows:

 * *N_validation*: counter of encounters
 * *Identity*: User ID within the first repeat database. Note that the ID of users change within each fold, based on the order of appearance. For instance, if user ID 63 was seen first, the ID of that user will be 1 in that fold.
 * Image path for each user's image (*Original_image* or *Sequential_image* refers to the same image). The user's ID in the original image path refers to the ID of the user in the TrainAll/TestAll/TrainTen/TestTen dataset. *Bin* and *Bin_image* refer to where the image will be found in the binned folders (for 5 folds). *Validation_image* is the new image title. For instance, for the above user, the previous image could be in bin 2, and be the second image of user 63, hence, the name of the image will be *63_2*. In the new fold, it will be *1_1* because its is the first user's first image. *N_original* refers to the original ID of the user in the IMDB-Wiki dataset.
 * *Height*: The estimated height of the user, and the standard deviation of the estimation (0.08).
 * *Time*: The time of interaction with the user, in the form \['HH:MM:SS', 'Day_of_the_week'\] Monday is day 1.

These repeats are then processed on Pepper robot's proprietary algorithms to obtain face recognition similarity scores, and gender and age estimates from images, as given in *after_recognition_naoqi* folder. The users are enrolled to the face recognition database as they are introduced. All images of the user are added to the face recognition dataset (which overwrites parameters after a certain number of images). The resulting multi-modal information is available in the following files:

 * *InitialRecognition.csv*: Contains the estimated user identity (*I_est*) and the estimated properties (face (*F*), gender (*G*) and age (*A*)) when a user is seen (*N* is the counter of encounters, i.e., number of the recognition). Estimated user identity is based on the recognition module (e.g., MMIBN explained in the next section). *F* is in the format \[*confidence_score_of_face_recognition*, \[ \['user_id_2', *similarity_score*\] , \['user_id_5', *similarity_score*\], ...\]\] sorted in descending order of similarity score for all users in the face dataset. *G* is in the format \[*estimated_gender*, *confidence_score_of_gender_recognition*\]. Note that gender recognition is binary (*Male* or *Female*) due to the gender recognition algorithm in NAOqi. *A* is in the format \[*estimated_age*, *confidence_score_of_age_recognition*\]. The *H* and *T* are taken from the *validation_info_fold.csv* file in the fold and correspond to *Height* and *Time*.
 * *RecogniserBN_data.csv*: If the user is new (i.e., the user is not previously encountered), the user is added to the face recognition database of NAOqi, and the face recognition, gender and age estimations are taken again on the same image. This file contains the corresponding *F*, *G*, and *A* parameters and the true identity of the user (*I*) and whether the user enrolled to the system in that encounter (*R* is 1) or not (*R* is 0). *H* and *T* are the same as in *InitialRecognition.csv*. If the user is not new, then the *N* entry for *InitialRecognition.csv* and *RecogniserBN_data.csv* will be the same.
 * *db_data.csv*: This file contains the new ID of the user (corresponding to the order in the *validation_info_fold.csv* file), the user name, gender (as taken from IMDB-Wiki dataset), height (as taken from the web), the time of interaction when the user enrolled (*times*), and *occurrence* which corresponds to \[*number_occurrences_of_user*, *number_of_images_taken_while_enrolling*, *number_total_images_of_user*\]. The *occurrence* is \[0,0,0\] as default.

Additional files:

 * *default*: The resulting face recognition database extracted from NAOqi.
 * *Analysis/Comparison.csv*: Compares user recognition model performance to face recognition. *I_real* is the true identity of the user, *I_est* is the estimated identity by the model, and *F_est* is the estimated identity by face recognition. *I_prob* is the posterior scores/probability scores for each identity starting with unknown user (ID 0) and the remaining IDs in ascending order (1,2,3..), *F_prob* is the face recognition similarity scores with the first entry 0 if highest similarity score is above that of the face recognition threshold (0.4 for NAOqi), and 0 otherwise. *Calc_time* is the time required for recognition (estimation and confirmation of identity) by the model. *R* represents whether the user is registered (enrolled), 0 if the user is already registered, 1 if the user is registering in the current recognition entry. *Quality* is the quality of the estimation (the difference between the highest posterior score and the second highest score, divided by the number of enrolled users). *Highest_I_prob* is the highest posterior (or probability) score for the model, and *Highest_F_prob* is the highest similarity score of face recognition.
 * *images*: Sorted images according to the recognition results. If the user is known and recognised correctly, the image will be under *Known_True*, or if that user is recognised as someone else, the image will be in *Known_False*, or if that user is recognised as a new user, the image will be in *Known_Unknown*; if the user is new, and recognised correctly as a new user, the image will be in *Unknown_True*, but if that user is recognised as a known user, the image will be in *Unknown_False*; if no face can be detected in the image, the image will be in *discarded*. Each image is named in the format *N*_*I*_*O*, where *N* is the number of recognition, *I* is the identity of the user for that fold, and *O* is the occurrence of that user. For instance, for the 40th recognition, the user 3 is seen for the second time, and incorrectly recognised as user 1, then the image *40_3_2.jpg* will be in *Known_False*. Note that for the *Test* folders (closed-set), *Unknown_False* and *Unknown_True* will always be empty, because all users are known (registered during *training* or *open-set*). For an example, you can look at the *images* folder in second fold of repeat 6 for D-Ten with Gaussian times (*N10_gaussianT*) in training set (*Training*) set (in *after_recognition_naoqi* folder) or trained models in *trained-models* release.

## User Recognition Evaluations on the Dataset

Multi-modal Incremental Bayesian Network (MMIBN) by Irfan et al. (2018; under review) and Extreme Value Machine (EVM) by Rudd et al. (2017) are evaluated on the Multi-modal Long-Term User Recognition Dataset. The results show that MMIBN models achieve significantly lower long-term recognition performance loss compared to the baseline face recognition (NAOqi) and EVM. For more details about the evaluations and the models, refer to Irfan et al. (under review). The *trained-models* contains the scripts, trained models and the results for reproducibility of the reported results in Irfan et al. (under review).

### Multi-modal Incremental Bayesian Network (MMIBN)


Multi-modal Incremental Bayesian Network (MMIBN) is the first user recognition method that can continuously and incrementally learn users, without the need for any preliminary training, for fully autonomous user recognition in long-term human-robot interactions. It is also the first method that combines a primary biometric (face recognition) with weighted soft biometrics (gender, age, height and time of interaction) for improving open world user identification in real-time human-robot interaction. In order to learn the changes in the user appearance, we extend MMIBN with online learning (MMIBN:OL) that adapts the likelihoods in the Bayesian network with incoming data. [https://agrum.gitlab.io/pages/pyagrum.html](pyAgrum) (Gonzalez et al., 2017) library is used for implementing the Bayesian network structure.

Clone the Multi-modal Incremental Bayesian Network and follow the instructions to build the libraries:

    $ git clone https://github.com/birfan/MultimodalRecognition.git

For reproducing the results in Irfan et al. (under review), replace *RecognitionMemory.py* with the one provided under *scripts*. Note that the MultimodalRecognition repository contains the latest code, and it may have slight changes with the code provided here. Run *run_KFoldCrossValidation.py* script to run the 11-repeats of 5-fold cross-validation on MMIBN and MMIBN:OL models. Make sure to move the folders (i.e., TrainAll, TestAll, TrainTen, TestTen, Repeated5fold folders, csv files and scripts) back to the main folder, since the scripts and datasets were in the main folder for creation/evaluation for the experiments and have been foldered for the sole purpose of visibility.

For more information about the models, see Irfan et al. (2018; under review).

#### Face Recognition Threshold Optimisation

In most face recognition algorithms, a threshold on the highest similarity score determines whether the user is known or new. We evaluated the NAOqi face recognition threshold with 0.01 increments between 0.0 and 1.0, and noticed a decrease in performance after 0.4. Hence, the optimal threshold is 0.4, because it is the highest threshold giving the lowest loss to decrease the fraction of incorrectly identified new users. The evaluations are provided in *face_recognition_threshold_optimisation.tar.gz* in *trained-models* release.

#### Optimisation of Parameters

Bayesian optimisation (through https://thuijskens.github.io/2016/12/29/bayesian-optimisation/ provided with *gp.py* script) is used to optimise the weights of the network and the threshold for the quality of the estimation. A total of 303 iterations is used for 5-fold cross-validation for each combination of the independent variables (i.e., for each normalisation method, MMIBN model and dataset). The parameters are optimised by minimising the long-term recognition performance loss on the training set (*WeightOptimisation.py* script). The results are provided in *bayesian_optimisation_norm_methods.tar.gz* in *trained-models* release. The *evidence_method* is *None* for the non-adaptive model (MMIBN), and *evidence* for online learning (MMIBN:OL). Hybrid normalisation combines the methods that achieve the lowest loss for each modality, and performs significantly better than other normalisation methods. The resulting optimised parameters for MMIBN and MMIBN:OL are available in *MMIBN_results_params_times.tar.gz*.

#### Cross-validation with Optimised Parameters

The scripts provided in the *mmibn* folder are the scripts used for the 11-fold cross-validation on the MMIBN models. The trained MMIBN and MMIBN:OL models and soft biometrics models (SB and SB:OL) on these datasets for the randomly chosen repeat (6), and the results in all repeats are available in *trained-models* release, as reported in Irfan et al. (under review).

### Extreme Value Machine

Extreme Value Machine (EVM, Rudd et al. (2017)) is a state-of-the-art open world recognition (i.e., incremental learning of new classes, in addition to recognising previously learned classes) method. To accept sequential and incremental data for online learning, we adapted EVM by adjusting its hyperparameters to use it as a baseline (as provided in *evm.py* file under *scripts*). In the original work, batch learning of 50 classes was used with an average of 63806 data points at each update, instead of a single data point that we used in this work. We compared MMIBN models with the performance of two EVM models: (a) EVM:FR, using NAOqi face recognition similarity scores as data (trained models in *EVM_face* in *trained-models* release), (b) EVM:MM using the same multi-modal data as in MMIBN (trained models in *EVM_all* in *trained-models* release). *EVM_soft* also provides results using only soft biometrics data (gender, age, height and time of interaction). The parameters of EVM (cover threshold and open-set threshold) are optimised for EVM:FR and EVM:MM and the optimum parameters and the corresponding results are provided in *EVM_scripts_params_times.tar.gz* in *trained-models* release. Note that *EVM-0.1.2.zip* contains the original code and the README to install necessary libraries. *evm_IMDB.py* script is used to evaluate EVM on the Multi-modal Long-Term User Recognition Dataset. Functions from *RecognitionMemory.py* and *crossValidation.py* are necessary to reproduce the results for EVM.

## License

The Multi-modal Long-Term User Recognition Dataset is released under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Multi-modal Incremental Bayesian Network is released under a GNU General Public License v3.0. 
In other words, the dataset is made available for academic research purpose only, and the scripts for user recognition allow modification and reuse only with attribution and releasing under the same license. The licenses are included with the data and the scripts.

## Contact

Irfan et al. (under review) contains more information about the dataset and the evaluations. For any other information on the dataset or the MMIBN, contact Bahar Irfan: bahar.irfan (at) plymouth (dot) ac (dot) uk.

## Acknowledgments

We would like to thank Pierre-Henri Wuillemin for his substantial help with pyAgrum, and Ethan M. Rudd for his suggestions in adapting the Extreme Value Machine for online recognition.

## References

 * Ethan M. Rudd, Lalit P. Jain, Walter J. Scheirer and Terrance E. Boult (2018), "The Extreme Value Machine" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 40, no. 3, pp. 762-768, [https://doi.org/10.1109/TPAMI.2017.2707495](DOI:10.1109/TPAMI.2017.2707495)
 * Christophe Gonzales, Lionel Torti and Pierre-Henri Wuillemin (2017), "aGrUM: a Graphical Universal Model framework", International Conference on Industrial Engineering, Other Applications of Applied Intelligent Systems, Springer, [https://doi.org/10.1007/978-3-319-60045-1_20](DOI:10.1007/978-3-319-60045-1_20)
 * Rasmus Rothe, Radu Timofte and Luc Van Gool (2015), "DEX: Deep EXpectation of apparent age from a single image", IEEE International Conference on Computer Vision Workshops (ICCVW)
 * Rasmus Rothe, Radu Timofte and Luc Van Gool (2018), "Deep expectation of real and apparent age from a single image without facial landmarks", International Journal of Computer Vision, vol. 126, no. 2-4, pp. 144-157, Springer, [https://doi.org/10.1007/s11263-016-0940-3](DOI:10.1007/s11263-016-0940-3)
